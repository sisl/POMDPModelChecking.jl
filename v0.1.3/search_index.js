var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"reachability/#Reachability-Solver","page":"Reachability Solver","title":"Reachability Solver","text":"","category":"section"},{"location":"reachability/","page":"Reachability Solver","title":"Reachability Solver","text":"Reachability problems in MDP or POMDP consists at computing the policy that maximizes the chances of reaching a desired set of states.  In constrained reachability problem, there is an additional constraint that a given set of states must be avoided. ","category":"page"},{"location":"reachability/","page":"Reachability Solver","title":"Reachability Solver","text":"The ReachabilitySolver solves reachability and constrained reachability problems in MDPs and POMDPs expressed using POMDPs.jls. It returns the policy that maximizes the probability of reaching a given set of states. For some policies, like value iteration policies, you also get the value function which is equivalent to the probability of reaching a desired state.","category":"page"},{"location":"reachability/","page":"Reachability Solver","title":"Reachability Solver","text":"In POMDPModelChecking.jl, the reachability solver express the problem as a planning problem. Given a POMDP model, it automatically defines a ReachabilityPOMDP such that solving this problem with traditionnal POMDP algorithms yields the solution to the reachability problem. With this approach, the ReachabilitySolver supports any kind of solvers from POMDPs.jl.","category":"page"},{"location":"reachability/#Reachability-in-MDPs","page":"Reachability Solver","title":"Reachability in MDPs","text":"","category":"section"},{"location":"reachability/","page":"Reachability Solver","title":"Reachability Solver","text":"Here is an example of solving a reachability problem in a grid world MDP. ","category":"page"},{"location":"reachability/","page":"Reachability Solver","title":"Reachability Solver","text":"using POMDPs\nusing POMDPModelChecking\nusing POMDPModels\nusing DiscreteValueIteration \n\nmdp = SimpleGridWorld(size=(10,10), terminate_from=Set([GWPos(9,3)]), tprob=0.5)\n\n# Reach the cell (10,1) and avoid cell (9, 3) in the gridworld\nsolver = ReachabilitySolver(reach=Set([GWPos(10,1)]),\n                            avoid = Set([GWPos(9, 3)]), \n                            solver = ValueIterationSolver(verbose=true))\n\npolicy = solve(solver, mdp)\n\n# Maximum probability of reaching (10, 1) while in state (1, 1)\nvalue(policy, GWPos(1, 1))","category":"page"},{"location":"reachability/","page":"Reachability Solver","title":"Reachability Solver","text":"In the case of MDP problems, we can visualize the resulting probability of success over the whole state space","category":"page"},{"location":"reachability/","page":"Reachability Solver","title":"Reachability Solver","text":"using POMDPTools\n\n# overwrite the usual gridworld colorscale because here \n# the value is a probability \nusing ColorSchemes\nfunction POMDPModels.tocolor(r::Float64)\n    minr = 0.0\n    maxr = 1.0\n    frac = (r-minr)/(maxr-minr)\n    return get(ColorSchemes.redgreensplit, frac)\nend\np = render(mdp, color = s->value(policy, s), policy=policy)\nusing Compose; nothing # hide\ndraw(SVG(\"gw_reachability.svg\"), p); nothing # hide","category":"page"},{"location":"reachability/","page":"Reachability Solver","title":"Reachability Solver","text":"(Image: )","category":"page"},{"location":"reachability/#Reachability-in-POMDPs","page":"Reachability Solver","title":"Reachability in POMDPs","text":"","category":"section"},{"location":"reachability/","page":"Reachability Solver","title":"Reachability Solver","text":"Using the same methodology, we can solve reachability problems in POMDPs. The interface is exactly the same, but the result is interpreted differently. POMDP solvers outputs a belief state policy and belief state value function. As a consequence, it gives the probability of reaching the reach set in a given belief state. ","category":"page"},{"location":"reachability/","page":"Reachability Solver","title":"Reachability Solver","text":"Instead of using ValueIterationSolver as the backend to solve the associated reward maximization problem, we use an approximate POMDP solver like SARSOP.","category":"page"},{"location":"reachability/","page":"Reachability Solver","title":"Reachability Solver","text":"The test folder of this package contains an implementation of gridworld where the agent only has a noisy observation of its position.","category":"page"},{"location":"reachability/","page":"Reachability Solver","title":"Reachability Solver","text":"using POMDPs\nusing POMDPModelChecking\nusing SARSOP\ninclude(joinpath(dirname(pathof(POMDPModelChecking)), \"../test/blind_gridworld.jl\"))\n\npomdp = BlindGridWorld(exit=GWPos(10, 1))\n\nsolver = ReachabilitySolver(reach=Set([GWPos(10,1)]),\n                            avoid = Set([GWPos(9, 3)]), \n                            solver = SARSOPSolver(verbose=true,precision=1e-2, timeout=5))\n\npolicy = solve(solver, pomdp)\n\nb0 = initialstate(pomdp)\nvalue(policy, b0)","category":"page"},{"location":"reachability/","page":"Reachability Solver","title":"Reachability Solver","text":"ReachabilitySolver","category":"page"},{"location":"reachability/#POMDPModelChecking.ReachabilitySolver","page":"Reachability Solver","title":"POMDPModelChecking.ReachabilitySolver","text":"ReachabilitySolver{S} <: Solver\n\nSolves reachability and constrained reachability problems in MDPs and POMDPs. It returns the policy that maximizes the probability of reaching a given set of states. It takes as input the set of states to reach and the set of states to avoid, as well as the underlying solver. Any solver from POMDPs.jl are supported.\n\nFields\n\nThe field are specified as keyword arguments to the solver.\n\nreach::Set{S} the set of states to reach\navoid::Set{S} the set of states to avoid\nsolver::Solver the underlying solver to use (default is ValueIterationSolver)\n\n\n\n\n\n","category":"type"},{"location":"#About","page":"About","title":"About","text":"","category":"section"},{"location":"","page":"About","title":"About","text":"POMDPModelChecking.jl is a package for solving model checking problems in POMDPs. It is part of the POMDPs.jl ecosystem. ","category":"page"},{"location":"","page":"About","title":"About","text":"","category":"page"},{"location":"model_checking/#Model-Checking-Solver","page":"Model Checking Solver","title":"Model Checking Solver","text":"","category":"section"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"Model checking consists of computing the maximum probability of satisfying a logical formula in a given state of an MDP, or in a given belief state of a POMDP. The approach in POMDPModelChecking.jl is to internally convert the problem into a reachability problem. This conversion is done automatically and supports any discrete state and discrete actions model. ","category":"page"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"The model checking solver can also be used as a way to solve POMDPs using a logical formula as input in place of the traditional reward function.","category":"page"},{"location":"model_checking/#Specifying-formulas","page":"Model Checking Solver","title":"Specifying formulas","text":"","category":"section"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"Formulas are expressed using linear temporal logic. We use Spot.jl to parse and manipulate such formulas.  ","category":"page"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"using Spot\nsafety = ltl\"!crash U goal\"\nmission = ltl\"F good_rock & F exit & G !bad_rock\"","category":"page"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"Those formulas read in plain english respectively as follows: \"Do not crash until you reach the goal\", \"Eventually pick up a good rock and eventually exit and never pick up a bad rock\". ","category":"page"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"Each ltl formula is composed of atomic propositions: crash, goal, good_rock... A POMDP problem writer must specify a labeling function that returns which proposition hold true in a given state. ","category":"page"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"One must implement the function POMDPModelChecking.labels(problem::M, s::S, a::A) where M is the problem type, S the state type of the problem, and A the action type. This function must return a tuple of symbols. Each symbol corresponds to the atomic propositions that hold true in this state.","category":"page"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"labels","category":"page"},{"location":"model_checking/#POMDPModelChecking.labels","page":"Model Checking Solver","title":"POMDPModelChecking.labels","text":"Returns the labels associated with state s  For each state, it should return a list of atomic proposition that evaluate to true, all the other propositions are assumed false. labels(mdp::M, s, a) where {M <: Union{MDP,POMDP}}\n\n\n\n\n\n","category":"function"},{"location":"model_checking/#Solving-model-checking-problems","page":"Model Checking Solver","title":"Solving model checking problems","text":"","category":"section"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"Given a POMDP or MDP model, a labeling function and a formula, one can solve a model checking problem using the ModelCheckingSolver. Its interface is similar to any other solver in POMDPs.jl.","category":"page"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"ModelCheckingSolver","category":"page"},{"location":"model_checking/#POMDPModelChecking.ModelCheckingSolver","page":"Model Checking Solver","title":"POMDPModelChecking.ModelCheckingSolver","text":"ModelCheckingSolver\n\nA probabilistic model checker for MDPs and POMDPs with LTL specification.  The solver takes as input an LTL formula and the underlying MDP/POMDP planning algorithm used to perform the model checking.  It supports any solver from POMDPs.jl.  Internally, this solver requires a discrete state and discrete action model.\n\nFields\n\nproperty::SpotFormula\nsolver::Solver any MDP/POMDP solver\ntolerance::Float64 = 1e-3\nverbose::Bool = true\n\n\n\n\n\n","category":"type"},{"location":"model_checking/#Examples","page":"Model Checking Solver","title":"Examples","text":"","category":"section"},{"location":"model_checking/#Rock-Sample","page":"Model Checking Solver","title":"Rock Sample","text":"","category":"section"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"Let's use the rock sample problem as an example.  The robot must pick up a good rock and then exit the environment. The model checking problem will be to compute the maximum probability of picking a good rock without ever picking up a bad rock and then coming back to the base.","category":"page"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"using POMDPs\nusing Spot\nusing POMDPModelChecking\nusing RockSample\nusing SARSOP\n\npomdp = RockSamplePOMDP{2}(map_size=(4,4), \n                        rocks_positions=[(2,3), (3,1)])\n\n\nprop = ltl\"F good_rock & G !bad_rock & F exit\" \n\n# Define the labeling function which tells which proposition hold true in a given state\n# For the rock sample problem, good_rock holds true if the robot is on a good rock location \n# and take the action `sample` (a=5)\n# similarly, bad_rock holds true if the robot samples a bad rock\n# The exit proposition is true if the robot reached a terminal state\nfunction POMDPModelChecking.labels(pomdp::RockSamplePOMDP, s::RSState, a::Int64)\n    if a == RockSample.BASIC_ACTIONS_DICT[:sample] && in(s.pos, pomdp.rocks_positions) # sample \n        rock_ind = findfirst(isequal(s.pos), pomdp.rocks_positions) # slow ?\n        if s.rocks[rock_ind]\n            return (:good_rock,)\n        else\n            return (:bad_rock,)\n        end\n    end\n    if isterminal(pomdp, s)\n        return (:exit,)\n    end\n    return ()\nend\n\nsolver = ModelCheckingSolver(property=prop,\n                             solver=SARSOPSolver(precision=1e-3, timeout=5), verbose=true)\npolicy = solve(solver, pomdp);","category":"page"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"We can visualize the policy as follows:","category":"page"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"using Random\nusing POMDPTools\nusing POMDPGifs\n\n# first simulate the product pomdp\nrng = MersenneTwister(2)\nup = DiscreteUpdater(policy.problem)\nb0 = initialize_belief(up, initialstate(policy.problem))\nhr = HistoryRecorder(max_steps=50, rng=rng)\nproduct_hist = simulate(hr, policy.problem, policy, up, b0);\n\n# create a new history with the pomdp state and action, to be replayed and visualized\nhist = SimHistory([(s=s.s, a=a) for (s, a) in eachstep(product_hist, \"(s,a)\")], \n                  discount(pomdp), nothing, nothing)\nimport Cairo\nmakegif(pomdp, hist, filename=\"rocksample.gif\", spec=\"(s,a)\");","category":"page"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"(Image: )","category":"page"},{"location":"model_checking/#Drone-Surveillance","page":"Model Checking Solver","title":"Drone Surveillance","text":"","category":"section"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"In this second example we use a drone surveillance problem where a drone must go from one corner to the other corner of a grid world while not being detected by a ground agent moving randomly.","category":"page"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"The property we want to satisfy is \"do not be detected until region b is reached\", which in LTL is given by !detected U b","category":"page"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"using POMDPs\nusing POMDPModelChecking\nusing Spot\nusing DroneSurveillance\nusing SARSOP\n\npomdp = DroneSurveillancePOMDP(size=(5,5), camera=PerfectCam())\n\nprop = ltl\"!detected U b\"\n\nfunction POMDPModelChecking.labels(pomdp::DroneSurveillancePOMDP, s::DSState, a::Int64)\n    if s.quad == pomdp.region_B\n        return (:b,)\n    elseif s.quad == s.agent && !(isterminal(pomdp, s))\n        return (:detetected,)\n    else\n        return ()\n    end\nend\n\nsolver = ModelCheckingSolver(property = prop, \n                             solver = SARSOPSolver(precision=1e-2, timeout=20), verbose=true)\n\npolicy = solve(solver, pomdp);\n\n\n## Simulation and rendering \nusing Random\nusing POMDPTools\nusing POMDPGifs\n\n# run the simulation in the product POMDP, policy.problem\nrng = MersenneTwister(3)\nup = DiscreteUpdater(policy.problem)\nb0 = initialize_belief(up, initialstate(policy.problem))\nhr = HistoryRecorder(max_steps=20, rng=rng)\nproduct_hist = simulate(hr, policy.problem, policy, up, b0);\n\n# create a new history with the pomdp state and action, to be replayed and visualized\nhist = SimHistory([(s=s.s, a=a) for (s, a) in eachstep(product_hist, \"(s,a)\")], discount(pomdp), nothing, nothing)\nimport Cairo\nmakegif(pomdp, hist, filename=\"drone_surveillance.gif\", spec=\"(s,a)\");\n","category":"page"},{"location":"model_checking/","page":"Model Checking Solver","title":"Model Checking Solver","text":"(Image: )","category":"page"}]
}
